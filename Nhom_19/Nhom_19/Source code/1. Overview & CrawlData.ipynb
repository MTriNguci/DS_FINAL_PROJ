{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Topic: Analyzing the cars sold on Carvago, the largest used car market with vehicles from all over Europe.\n",
    "\n",
    "As Carvago stands out as the largest platform for used car transactions in Europe, our capacity to extract a wealth of valuable information from this website is substantial. We meticulously collect a diverse range of data, including pricing details, detailed technical specifications, top equipment features, and ratings. Leveraging this comprehensive dataset, we aim to gain a holistic understanding of the European used car market. Our objective is to delve into various aspects such as the market share of electric versus traditional fuel vehicles, the performance strengths of engines across different segments, and the distinctive attributes that contribute to the top-rated equipment of a vehicle. We seek to analyze the impact of these equipment features on the overall evaluation of a car.\n",
    "\n",
    "Furthermore, we are in the process of developing a predictive pricing model to assist users in their quest for finding accurate and reliable pricing information. This model will contribute to enhancing the user experience by providing valuable insights into the pricing dynamics of the used car market, thereby aiding individuals in making informed decisions.\n",
    "\n",
    "\n",
    "## Lisence: \n",
    "The data is collected from [Carvago](https://carvago.com/) for educational and research purposes. All information used complies with the regulations and policies of Carvago. The user of this data is responsible for ensuring that their usage complies with all relevant laws and regulations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before crawling information about cars, we need to observe how the website operates. \n",
    "\n",
    "We notice that when accessing the 'https://carvago.com/cars' page, a list of cars is displayed, sorted by the newest ones. At the top right corner, there is a navigation bar to go to the next page. Through observation, we find that each page displays 20 cars, and upon inspecting the source code, we see that the page contains 20 links leading to the details page for each car.\n",
    "\n",
    "Additionally, we can navigate between pages using the following URL format: https://carvago.com/cars?page={i}, where i represents the page we want to move to.\n",
    "\n",
    "Before crawling detailed information about the cars, we need to obtain the URLs for each car. Therefore, we will iterate through these pages and crawl the links of the cars using the HTML parsing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawl URL của 3000 sản phẩm và lưu vào file links.txt\n",
    "def get_id(url):\n",
    "\n",
    "    # Mở file để ghi\n",
    "    with open('data/links.txt', 'w') as f:\n",
    "\n",
    "        # Mỗi trang xe có link của 20 xe\n",
    "        for i in range(1, 151):\n",
    "            print(i)\n",
    "            response = requests.get(url.format(i))\n",
    "\n",
    "            # Trích xuất phần cần thiết của mã nguồn trang web\n",
    "            text = response.text\n",
    "\n",
    "            # Tìm tất cả các cặp \"id\" và \"slug\" trong phần đã trích xuất\n",
    "            matches = re.findall(r'\"id\":\"(\\d{8})\",\"slug\":\"([\\w-]+)\",', text)\n",
    "\n",
    "            for match in matches:\n",
    "                id, slug = match\n",
    "                # Ghi vào file thay vì in ra\n",
    "                f.write(f'https://carvago.com/car/{id}/{slug}\\n')\n",
    "\n",
    "# Sử dụng hàm với url đã được chỉnh sửa để chứa số trang\n",
    "#get_id('https://carvago.com/cars?page={}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully crawling links to detailed information for each car, we will save all of them to the file **data/links.txt** in preparation for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl detail information of car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the paths to the details of the cars to be crawled, we will implement a function to crawl the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(text, pattern):\n",
    "    \"\"\"\n",
    "    Trả về tuple chứa các thông tin được trích xuất từ text theo pattern\n",
    "    \"\"\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return None, None, None, None \n",
    "\n",
    "def extract_car_info(url):\n",
    "    \"\"\"\n",
    "    url: đường đẫn đến thông tin của 1 xe\n",
    "    return df: trả về dataframe chứa thông tin của 1 xe\n",
    "    Một số trường hợp như xe đã không còn bán nữa,... ta sẽ in ra thông báo lỗi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            if (soup.title.text == 'Carvago.com | Buying a used car online. Warranty and home delivery.'):\n",
    "                print('web nolonger available, car has been sold!!!!!')\n",
    "                return None\n",
    "            print('200 OK')\n",
    "\n",
    "            car_info_divs = soup.find_all('div', class_='css-1b3bvbl')  # Adjust the class if needed\n",
    "\n",
    "            carname = \"\"\n",
    "\n",
    "            for car_info_div in car_info_divs:\n",
    "                attribute_name_div = car_info_div.find('div', class_='css-f6f0k1')\n",
    "                if attribute_name_div is not None:\n",
    "                    attribute_name = attribute_name_div.text.strip()\n",
    "\n",
    "                    if attribute_name == 'Model':\n",
    "                        car_link_div = car_info_div.find('div', class_='css-qres8i')\n",
    "                        if car_link_div is not None:\n",
    "                            car_link_tag = car_link_div.find('a')\n",
    "                            if car_link_tag is not None:\n",
    "                                car_link = car_link_tag['href']\n",
    "                                carname = car_link.split('/')[2:]  # Extract the last part of the URL\n",
    "                                carname = \" \".join(carname)\n",
    "            \n",
    "            # Lấy tên thông tin (ví dụ: 'Make', 'Model', ...)\n",
    "            keys = []\n",
    "            keys.append('CARNAME')\n",
    "            keys.append('ID')\n",
    "            keys.extend([div.get_text() for div in soup.find_all('div', {'class': 'css-f6f0k1'})])\n",
    "\n",
    "            # Lấy giá trị thông tin (ví dụ: 'Chevrolet', 'Orlando', ...)\n",
    "            values = []\n",
    "            values.append(carname)\n",
    "            values.append(url.split('/')[4])\n",
    "            for key, div in zip(keys[2: ], soup.find_all('div', {'class': 'css-1b3bvbl'})):\n",
    "                if key == 'VIN':\n",
    "                    if div.find('div', {'class': 'css-10odsig'}) is not None:\n",
    "                        value = div.find('div', {'class': 'css-10odsig'}).get_text()\n",
    "                    else:\n",
    "                        value = div.find('div', {'class': 'css-1yu0dva'}).get_text()\n",
    "                else:\n",
    "                    value = div.find('div', {'class': 'css-qres8i'}).get_text()\n",
    "                values.append(value)\n",
    "\n",
    "            #Do thông tin của consumption nằm ở thẻ khác nên ta sẽ xử lí riêng cho nó\n",
    "            consumption = [div.get_text() for div in soup.find_all('div', {'class': 'css-c55dw0'})]\n",
    "            keys.append('Consumption')\n",
    "            values.append('; '.join(consumption))\n",
    "\n",
    "            # Lấy giá tiền và đơn vị tiền tệ\n",
    "            price_pattern = r'\"price\":(\\d+),\"price_currency\":\\{\"id\":(\\d+),\"name\":\"(.*?)\",\"const_key\":\"CURRENCY_(.*?)\"\\}'\n",
    "            price, currency_id, currency_name, currency_const_key = get_data(response.text, price_pattern)\n",
    "\n",
    "            if price is not None:\n",
    "                keys.append('Price')\n",
    "                values.append(price)\n",
    "\n",
    "            if currency_name is not None:\n",
    "                keys.append('Currency')\n",
    "                values.append(currency_name)\n",
    "\n",
    "            # Lấy các tính năng nổi bật (featured tags)\n",
    "            tags_pattern = r'\"featured_tags\":(\\[.*?\\])'\n",
    "            featured_tags = json.loads(get_data(response.text, tags_pattern)[0])\n",
    "            keys.append('Tags')\n",
    "            \n",
    "            #Chuyển các giá trị trong các thẻ thành một string\n",
    "            values.append('; '.join([tag['name'] for tag in featured_tags]))\n",
    "\n",
    "            df = pd.DataFrame([values], columns=keys)\n",
    "\n",
    "            return df\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to get the webpage at {url} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text file obtained from the previous step, iterate through all the links, and call the function to crawl detailed information for each link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read links from data.txt\n",
    "with open('data/link2.txt', 'r') as file:\n",
    "    links = file.read().splitlines()\n",
    "\n",
    "list_dataframe = []\n",
    "data = pd.DataFrame()\n",
    "# Loop through each link and extract information\n",
    "i = 0\n",
    "for link in links:\n",
    "    print(i, '/', len(links))\n",
    "    car_info = extract_car_info(link)\n",
    "    if (car_info is None):\n",
    "        print(f\"Skipping link due to error\")\n",
    "        continue  \n",
    "    list_dataframe.append(car_info)\n",
    "\n",
    "\n",
    "temp_df = list_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_duplicates(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "list_dataframe = [rename_duplicates(df) for df in list_dataframe]\n",
    "\n",
    "\n",
    "all_columns = set().union(*[set(df.columns) for df in list_dataframe])\n",
    "\n",
    "list_dataframe = [df.reindex(columns=all_columns, fill_value=np.nan) for df in list_dataframe]\n",
    "\n",
    "crawldata = pd.concat(list_dataframe, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the crawled dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the number of rows we obtained does not match the quantity of crawled links. This discrepancy arises from the fact that some items have been removed from the website, but their corresponding detailed information links have not been taken down. It's only when we click on these links that we discover the product is out of stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to data/products.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawldata.to_csv(\"data/crawl_data.csv\",header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
