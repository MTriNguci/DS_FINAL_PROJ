{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Read data and create a column `Year` to help increase a feature for training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a column `Year` - distance from First Registration's Year of the car to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read data from csv .\n",
    "# initially (before presentation, there is only from cleaned_data.csv)\n",
    "\n",
    "data_1 = pd.read_csv(\"data/cleaned_data.csv\",index_col=\"ID\")\n",
    "data_2 = pd.read_csv(\"data/cleaned_add_data.csv\", index_col=\"ID\")\n",
    "data_2[\"First registration\"] = pd.to_datetime(data_2[\"First registration\"])\n",
    "data = pd.concat([data_1, data_2])\n",
    "\n",
    "data[\"First registration\"] = pd.to_datetime(data[\"First registration\"])\n",
    "data[\"Year\"] = 2023 - data[\"First registration\"].dt.year \n",
    "data.drop(columns=[\"First registration\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Find object columns and low cardinality columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our data have many categorical columns, we have to do one hot encoding for training model. Before handling one-hot, we have to:\n",
    "- Find categorical (object) columns.\n",
    "- Find columns that have low cardinality.\n",
    "\n",
    "Why have to find columns that have low cardinality?\n",
    "- For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (data.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "low_cardinality_cols = [col for col in object_cols if data[col].nunique() < 10]\n",
    "# low_cardinality_cols.append(\"Make\")\n",
    "print(\"Low cardinality col:\")\n",
    "print (low_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Find tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due to the column `Tags` is a multiple value column, we have to choose which tag (special function) to do one hot encoding and put to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_hot_tags =  data[[\"Price(EUR)\",\"Tags\"]].copy()\n",
    "# scaler = RobustScaler()\n",
    "# # Fit và transform dữ liệu\n",
    "# one_hot_tags['Price(EUR)'] = scaler.fit_transform(one_hot_tags[['Price(EUR)']]) \n",
    "\n",
    "# Get each tag in each multiple value row\n",
    "tags = one_hot_tags['Tags'].str.split('; ', expand=True)\n",
    "\n",
    "# Stack to make the DataFrame long, then get_dummies and group by index before summing\n",
    "get_dummy = pd.get_dummies(tags.stack()).groupby(level=0).sum()\n",
    "\n",
    "# Join the one-hot encoded DataFrame back to the original DataFrame\n",
    "one_hot_tags = one_hot_tags.join(get_dummy)\n",
    "\n",
    "# Drop the `Tags` because we dont need it anymore\n",
    "one_hot_tags = one_hot_tags.drop(\"Tags\",axis=1)\n",
    "\n",
    "# Calculate correlation with 'Price(EUR)'\n",
    "correlation = one_hot_tags.corr()['Price(EUR)']\n",
    "\n",
    "# Get the top 10 tags with highest correlation with 'Price(EUR)'\n",
    "top_10_corr_tags = correlation[correlation >= 0.25].index\n",
    "\n",
    "# Select only the top 10 tags with highest correlation with 'Price(EUR)'\n",
    "one_hot_tags = one_hot_tags[top_10_corr_tags].drop([\"Price(EUR)\"],axis=1)\n",
    "\n",
    "one_hot_tags.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Find top Car Manufacturers that have high correlation rate with Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_Make = pd.get_dummies(data['Make'])\n",
    "data_encoded = data[['Price(EUR)']].join(one_hot_Make)\n",
    "\n",
    "\n",
    "correlation = data_encoded.corr()['Price(EUR)']\n",
    "\n",
    "#LẤy ra top 8 make có tương quan cao nhất > 0.25\n",
    "top_10_Make = correlation[correlation >= 0.25].index\n",
    "top_10_Make = top_10_Make.drop(\"Price(EUR)\")\n",
    "\n",
    "one_hot_Make = one_hot_Make[top_10_Make]\n",
    "one_hot_Make['Others'] = (one_hot_Make.sum(axis=1) == 0)\n",
    "one_hot_Make.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train and test a model:\n",
    "    - First we do some simple step to get **train set** and **test set**.\n",
    "    - Second, we have to handle categorical features - one hot encoding.\n",
    "- Last update (After presentation): \n",
    "    - we add a test set to test the K-neighbour Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [cname for cname in data.columns if \n",
    "                data[cname].dtype in ['int64', 'float64', 'int32']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "\n",
    "X = data[my_cols].copy().reset_index(drop=True)\n",
    "one_hot_tags = one_hot_tags.reset_index(drop=True)\n",
    "one_hot_Make = one_hot_Make.reset_index(drop=True)\n",
    "\n",
    "X = pd.concat([X, one_hot_tags, one_hot_Make], axis=1)\n",
    "y = X[\"Price(EUR)\"].copy()\n",
    "X.drop([\"Price(EUR)\"], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                      train_size=0.5, test_size=0.5,\n",
    "                                                      random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create a PipeLine for Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we have to define transformer for the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer:\n",
    "- Preprocess Categorical columns.\n",
    "- Handle outliers in numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('robustscaler', RobustScaler(with_centering=False)),\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Exclude 'Price(EUR)' from numerical_cols\n",
    "numerical_cols_to_transform = [col for col in numerical_cols if col != 'Price(EUR)']\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols_to_transform),\n",
    "        ('cat', categorical_transformer, low_cardinality_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second: Fit model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', RandomForestRegressor(n_estimators=250))\n",
    "                             ])\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_test, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', KNeighborsRegressor(n_neighbors=5))\n",
    "                             ])\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_test, preds)\n",
    "print('MAE:', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Cross Validation & Adjust Parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tìm n_estimator cho RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def get_score(n_estimate):\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', RandomForestRegressor(n_estimators = n_estimate,\n",
    "                                                              random_state=0))\n",
    "                             ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X_train, y_train ,\n",
    "                                    cv=5,\n",
    "                                    scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "results = {}\n",
    "for i in range(1,8):\n",
    "    print(f'--------{50*i} estimators--------')\n",
    "    mae = get_score(50*i)\n",
    "    print(f'MAE: {mae}')\n",
    "    results[50*i] = mae\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best k for KNN ( After presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def get_score(n):\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', KNeighborsRegressor(n_neighbors=n))\n",
    "                                ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X_train, y_train ,\n",
    "                                    cv=5,\n",
    "                                    scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "results = {}\n",
    "for i in range(1,20):\n",
    "    mae = get_score(i)\n",
    "    results[i] = mae\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()\n",
    "\n",
    "#In ra giá trị nhỏ nhất và mae tương ứng\n",
    "min(results.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, with **n_estimators** = 150, our random forest model seems result the most optimal MAE.\n",
    "- With **n_neighbors** = 4, our KNN model seems result the most optimal MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe line for deploy model\n",
    "my_pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', RandomForestRegressor(n_estimators = 250,\n",
    "                                                              random_state=0))\n",
    "                             ])\n",
    "\n",
    "my_pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "my_pipeline_knn = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', KNeighborsRegressor(n_neighbors=4))\n",
    "                                ])\n",
    "\n",
    "my_pipeline_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Train model, Test and make Prediction from User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def input_car_info():\n",
    "    # Initialize an empty dictionary to hold user input\n",
    "    car_info = {}\n",
    "\n",
    "    # List all the features\n",
    "    features = ['Interior color', 'Interior material', 'Doors', 'Fuel', 'Transmission', \n",
    "                'Drive type', 'Emission class', 'Condition', 'Seats', 'Power(kW)', \n",
    "                'CO2 emissions(g/km)', 'Mileage(km)', 'Consumption(l/100km or kWh/100km)', \n",
    "                'Engine capacity(ccm)', 'Previous owners', 'Air suspension', \n",
    "                'Ventilated front seats', 'Electric adjustable front seats', \n",
    "                'Digital cockpit', 'Burmester audio', 'Heated rear seats', \n",
    "                'Laser headlights', 'Adaptive cruise control']\n",
    "\n",
    "    # Ask the user to input values for each feature\n",
    "    for feature in features:\n",
    "        value = input(f\"Please enter the {feature} of the car: \")\n",
    "        car_info[feature] = [value]  # Use a list here because pd.DataFrame expects a list\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    X_input = pd.DataFrame(car_info)\n",
    "\n",
    "    return X_input\n",
    "\n",
    "# Call the function to get user input and create X_input\n",
    "# X_input = input_car_info()\n",
    "# X_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = my_pipeline.predict(X_input)\n",
    "# preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deploy \n",
    "import joblib\n",
    "joblib.dump(my_pipeline_rf, 'deploy/model_rf.pkl')\n",
    "print(\"Model 1 dumped!\")\n",
    "\n",
    "joblib.dump(my_pipeline_knn, 'deploy/model_knn.pkl')\n",
    "print(\"Model 2 dumped!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
